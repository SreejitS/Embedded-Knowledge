#### Why Mutual Exclusion Mechanisms are Needed?
- Concurrent threads within an RTOS are very lightweight and all run in the SAME address space.
- This is unlike in the big desktop or server-style operating systems, such as Windows or Linux, where different processes cannot easily share variables and memory-mapped registers, because they run in SEPARATE address spaces.
- In RTOS environment, sharing of variables and hardware registers among thread is allowed as in RTOS does nothing to prevent it.
- *Note: the C compiler is completely unaware of the context-switch magic and the changes to the stack pointer that happen under the covers. Therefore, the compiler treats the thread-functions the same way as all other functions in the program, so they CAN access any variables and all other memory and hardware registers that are visible to them.*
- Sharing is the sources of conflicts i.e. race conditions and *mutual exclusion mechanisms* ensure mutually exclusive access to shared resources.
- *Race conditions can happen not only between ISR and main code, but also between threads accessing shared resource in a pre-emptive RTOS kernel*
- *Any form of mutual exclusion will cause slight delays*

#### Note for Race Conditions
- In main plus ISR codes, we understand that an interrupt after the read, but before the write for a read-modify-write, may write old value rather than the new value updated in the ISR handler code
- Similarly for pre-emptive RTOS kernel, low priority threads using the read-modify-write sequence could be pre-empted by a higher priority thread, which changes the value in the shared resource, and when the thread resumes, it writes back the old value, reverting the changes done in the high priority thread. *Usually a telltale sign is that the changes to a different part of the shared resource happens in the same CPU instruction i.e. during the write part.*

#### Mechanisms for mutual exclusion
##### Critical Sections
- The first mechanism for avoiding such race conditions is simply disabling interrupts around the code that touches the shared resource. This works in RTOS as well, because disabling interrupts cuts off the CPU completely from the external world, so preemption cannot happen. 
- This simplistic unconditional interrupt disabling and enabling around the critical section of code has at least two drawbacks.
	- First, it is inconsistent with the interrupt disabling policy of the RTOS. If the kernel implements the "zero-latency" policy by disabling interrupts selectively i.e. only up to a certain level of interrupt priority then this simplistic critical section disables all interrupts indiscriminately, and thus introduces additional latency for interrupts that should never be disabled. Basically makes the kernel unaware interrupts as kernel aware as RTOS will now have affect on the interrupt latency
	- And second, sometimes you might need to nest critical sections. For example, if you hide a critical section inside a function call, and then call this function also from a critical section established already. If your critical is not designed to nest, you will re-enable interrupts prematurely, and you might create a race condition in the piece of code that should be protected by a critical section, but isn't.
	  ![[nesting-critical-sections.JPG]]

- Generally RTOS kernel provides a critical section mechanism that is both consistent with the "zero-latency" interrupt disabling policy and that allows critical sections to nest.
- **In summary, critical section is a very powerful mutual exclusion mechanism that the RTOS itself uses to protect its own internal variables from race conditions. You can use it as well, but exactly because the mechanism is so powerful, it is applicable only to very short sections of code. Anything that takes more than a few microseconds, meaning only a handful of machine instructions, is just too long and can increase the interrupt latency in your system.**

##### Semaphore
- For longer pieces of codes accessing a common resource, semaphore could be used to grant access in a serial fashion. The common code is surrounded by a semaphore-wait in the beginning and a semaphore-signal at the end
- *Problem with this:*  Unbounded Priority Inversion: The fundamental issue here is that the semaphore is just unaware of the priorities of threads that it blocks, so nothing is done to prevent priority inversion from happening. This should be actually not that surprising, because semaphores have been invented in the era of timesharing systems, where the notion of thread priority was simply not used.
- **It turns out that the classic semaphore, while still applicable to synchronizing threads, is NOT a good mechanism for mutual exclusion in priority-based systems.**
![[sema.JPG]]


##### Selective Scheduler Locking up to the specified priority ceiling
- The scheduler is locked for the period of time the shared resource is being accessed, with an input parameter of the priority ceiling. This ceiling denotes the level up to which you lock the scheduler, meaning that all threads below or equal to the ceiling won't be scheduled, but any threads above the ceiling are scheduled as usual. Of course, the ISRs, that run above all threads, are not affected at all, so there is no impact on the interrupt latency.
- Ceiling priority must be at least as high as the priority of the highest-priority thread that uses the shared resource.
- *Note:  selective scheduler locking is a NON-BLOCKING mutual exclusion mechanism, similar as critical section was non-blocking. Except now, you will only prevent scheduling of threads up to the specified priority ceiling, while higher-priority threads and all interrupts will continue running undisturbed.*
- *Note for more understanding: When a low priority thread gains the lock, then any thread which does not access the share resource could have ran unbounded in case of semaphore during priority inversion due to a even higher priority thread being blocked by the lower one, would not be scheduled as it's priority is lower than the ceiling*
- **The only limitation of the selective scheduler locking is that the thread cannot block while holding the lock. Blocking while accessing a shared resource, such as calling the blocking-delay or the semaphore-wait APIs, is considered a bad practice and should be avoided.**
- **In summary, selective scheduler locking is a very effective, non-blocking mutual exclusion mechanism, which prevents unbounded priority inversion.**

![[schduler-lock.JPG]]

##### Mutex
- Don't think of a mutex as a kind of a semaphore.
- Think of a mutex as an RTOS object specifically designed for protecting resources shared among concurrent threads in the most generic case, where threads might block while accessing the shared resource.
- The protected section of the code starts with the `lock` call, at which point the thread becomes the owner of the mutex.
- The protection ends with the `unlock` call, at which point the thread relinquishes the ownership of the mutex.
- These are implemented using the following two methods
	- `Priority-Ceiling Protocol` - Here, the thread acquiring the mutex is promoted to the priority ceiling value, which is generally initialized to the one priority greater than the thread accessing the shared resource. This makes it the highest priority and runs till the mutex is released. 
![[priority-ceiling.JPG]]
	- `Priority-Inheritance Protocol` - Here, the thread's priority is not changed, but if a higher priority thread is waiting for mutex to be released, then the lower priority thread is automatically promoted to the higher priority. *The low-priority thread INHERITS the priority of the high-priority contender for the resource.*
![[priority-inheritance.JPG]]

- The pros and cons of the above two mutexes:
	-  Because there could be a chance of the initial preemption by the medium-priority thread, the high-priority thread runs and finishes later under priority-inheritance and so it runs a higher risk of missing its deadline.
	- Another big disadvantage of priority-inheritance is that it typically leads to many more expensive context switches than priority ceiling. In above example 2 switches vs 4 of them.
	- Priority inheritance is much more complex to implement correctly inside the RTOS.
	-  Priority ceiling has much simpler timing analysis for hard-real time systems